{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d4bd2c3-f61f-4b62-a864-149bca38c39f",
   "metadata": {},
   "source": [
    "## This is a file to test out web scraping the 2023 OHW projects page and return a csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47772ba5-0026-4f07-9335-2170cd4cac00",
   "metadata": {},
   "source": [
    "Import necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21650a28-97af-422b-92f9-70b7bf7de55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # used to get the data from the url\n",
    "from bs4 import BeautifulSoup # used to convert url request data to a readable and workable format\n",
    "import re # used to create a regex function for splitting strings (for help with regex visit: https://regexr.com/)\n",
    "import pandas as pd # used for many data science/analytical things but in this case, used to make a neat csv "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c5469a-b539-4ea4-b6da-b2027dd57a8e",
   "metadata": {},
   "source": [
    "##### Get the content from the above url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4149a07-e9d9-4c89-bc38-6212f9318056",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_content(url: str):\n",
    "    # Making a GET request\n",
    "    r = requests.get(url)\n",
    "    # use BeautifulSoup to make it pretty\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec3a2ea-1346-4592-be85-9792fbf2cb25",
   "metadata": {},
   "source": [
    "- `projects_html_split` is created from `projects_html` where we split the whole string using the regular expression (regex) `[<>]` which splits on all characters between the `[]`. The `filter` function with `None` as the first parameter filters out any list objects that are `None` or empty strings. Finally, we convert the filter result to a list.\n",
    "- Next create an empty list called `projects_list`\n",
    "- Now a `for` loop is used to loop through each `string` in `projects_html_split`\n",
    "    - If the first character of the string (`string[0]`) is a digit (in this case all project names start with #.), proceed\n",
    "    - Then strip out those numbers using a regex formula `[\\d.]`. `\\d` matches any digit character (0-9)\n",
    "    - Now append the new string to the `projects_list` list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac078e63-a671-4abd-ba00-d78fb35f5517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_projects_list(soup, tag):\n",
    "    projects_html = soup.find_all(tag)\n",
    "    projects_html_split = list(filter(None, re.split(r\"[<>]\", str(projects_html))))\n",
    "    projects_list = []\n",
    "    try:\n",
    "        for string in projects_html_split:\n",
    "            if string[0].isdigit():\n",
    "                stripped_string = re.sub(r'[\\d.]', '', str(string)).strip()\n",
    "                projects_list.append(stripped_string)\n",
    "            elif string[0:8] == 'Project:':\n",
    "                projects_list.append(string)\n",
    "        if projects_list == []:\n",
    "            raise Exception('resulting list is empty')\n",
    "    except Exception:\n",
    "        regex = re.compile(r'\\b[A-Z]')\n",
    "        for string in projects_html_split:\n",
    "            if regex.match(string):\n",
    "                projects_list.append(string)\n",
    "    return projects_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3447fbc2-6614-4f06-8f09-896b61495b70",
   "metadata": {},
   "source": [
    "- `github_urls_html_split` is created from `github_urls_html` where we split the whole string using the regular expression (regex) `[\"]` which splits on all characters between the `[]`. The `filter` function with `None` as the first parameter filters out any list objects that are `None` or empty strings. Finally, we convert the filter result to a list.\n",
    "- Next create an empty list called `github_urls_list`\n",
    "- Now a `for` loop is used to loop through each `string` in `github_urls_html_split`\n",
    "    - If the string `https://github.com` is contained with the string we are looking at, proceed \n",
    "    - Now append the new string to the `github_urls_list` list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f9ec8af-214b-4728-8651-e2476ed62225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_github_url_list(soup, tag, flag=None, **kwargs):\n",
    "    if len(kwargs.items()) != 0:\n",
    "        for key, value in kwargs.items():\n",
    "            if key == 'tag2':\n",
    "                github_urls_html = soup.find_all(tag, value)\n",
    "    else:\n",
    "        github_urls_html = soup.find_all(tag)\n",
    "    github_urls_html_split = list(filter(None, re.split(r'[\"<>]', str(github_urls_html))))\n",
    "    github_urls_list = []\n",
    "    for string in github_urls_html_split:\n",
    "        if 'https://github.com' in string:\n",
    "            if 'proj' in string or 'ohw18' in string:\n",
    "                github_urls_list.append(string)\n",
    "                # now remove any duplicates\n",
    "                github_urls_list_final = []\n",
    "                [github_urls_list_final.append(x) for x in github_urls_list if x not in github_urls_list_final]\n",
    "    if github_urls_list_final == []:\n",
    "        raise Exception('resulting list is empty')\n",
    "    return github_urls_list_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a7d593f-3089-4c49-9d49-25abd5b4e59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_2023 = 'https://oceanhackweek.org/ohw23/projects/projects_thisyear'\n",
    "url_2022 = 'https://oceanhackweek.org/ohw22/projects/projects_thisyear.html'\n",
    "url_2021 = 'https://oceanhackweek.org/ohw-resources/projects/projectlist/'\n",
    "url_2020 = 'https://oceanhackweek.org/ohw21/projects_2020.html'\n",
    "url_2019 = 'https://oceanhackweek.org/ohw19/projects_2019.html'\n",
    "url_2018 = 'https://oceanhackweek.org/ohw2018/projects.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8679c148-1928-40b1-841e-fb9fad31a714",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soup_2023 = get_url_content(url_2023)\n",
    "soup_2022 = get_url_content(url_2022)\n",
    "soup_2021 = get_url_content(url_2021)\n",
    "soup_2020 = get_url_content(url_2020)\n",
    "soup_2019 = get_url_content(url_2019)\n",
    "soup_2018 = get_url_content(url_2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8fcdab1-8376-43af-8fa9-a71d4e713707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projects_list_2023 = get_projects_list(soup_2023, tag = 'h3')\n",
    "projects_list_2022 = get_projects_list(soup_2022, tag = 'h2')\n",
    "projects_list_2021 = get_projects_list(soup_2021, tag = 'h2')\n",
    "projects_list_2020 = get_projects_list(soup_2020, tag = 'h3')\n",
    "projects_list_2019 = get_projects_list(soup_2019, tag = 'h3')\n",
    "projects_list_2018 = get_projects_list(soup_2018, tag = 'p')\n",
    "len(projects_list_2023 + projects_list_2022 + projects_list_2021 + projects_list_2020 + projects_list_2019 + projects_list_2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee87a226-f395-4b47-9c73-8c251e818f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "github_urls_list_2023 = get_github_url_list(soup_2023, tag=\"a\", tag2=\"github reference external\")\n",
    "print(len(github_urls_list_2023))\n",
    "print(len(projects_list_2023))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b614c9ff-32e9-4ebf-8b28-1b7a7dd8ee6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "20\n",
      "20\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "github_urls_list_2022 = get_github_url_list(soup_2022, tag=\"a\", tag2=\"github reference external\")\n",
    "print(len(github_urls_list_2022))\n",
    "print(len(projects_list_2022))\n",
    "# Need to fill in a few gaps\n",
    "# Firstly project #8 doesn't have a linked github repo\n",
    "# Secondly github link for #20 https://github.com/oceanhackweek/ohw22-proj-video-data-processing\n",
    "github_urls_list_2022.insert(7, 'None')\n",
    "github_urls_list_2022.append('https://github.com/oceanhackweek/ohw22-proj-video-data-processing')\n",
    "print(len(github_urls_list_2022))\n",
    "print(len(projects_list_2022))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "475ae283-917f-4324-9381-153b3b399878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "11\n",
      "11\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "github_urls_list_2021 = get_github_url_list(soup_2021, tag=\"a\")\n",
    "print(len(github_urls_list_2021))\n",
    "print(len(projects_list_2021))\n",
    "# the first url isn't a project link\n",
    "github_urls_list_2021.remove(github_urls_list_2021[0])\n",
    "# Secondly github link for Pull/Hack all ocean data repositories into a global searchable resource is https://github.com/oceanhackweek/metadata-repository\n",
    "github_urls_list_2021.insert(4, 'https://github.com/oceanhackweek/metadata-repository')\n",
    "print(len(github_urls_list_2021))\n",
    "print(len(projects_list_2021))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d847df20-0063-4f96-a2fc-63d17bf187c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "8\n",
      "8\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "github_urls_list_2020 = get_github_url_list(soup_2020, tag=\"a\")\n",
    "print(len(github_urls_list_2020))\n",
    "print(len(projects_list_2020))\n",
    "# github link for Project: Co-locators expansion is https://github.com/ioos/colocate\n",
    "github_urls_list_2020.append('https://github.com/ioos/colocate')\n",
    "print(len(github_urls_list_2020))\n",
    "print(len(projects_list_2020))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71674305-3f7c-4770-9074-46ebb1f5d3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "11\n",
      "11\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "github_urls_list_2019 = get_github_url_list(soup_2019, tag=\"a\")\n",
    "print(len(github_urls_list_2019))\n",
    "print(len(projects_list_2019))\n",
    "# github link for Project: Isopy is https://github.com/oceanhackweek/DataAccess/tree/master/isopy\n",
    "# Project: Modeling Volcano Deformation at Axial Seamount doesn't have a linked github repo\n",
    "# github link for Project: Working with Chlorophyll Data from the Cloud is https://github.com/oceanhackweek/DataAccess/tree/master/Chlorophyll\n",
    "# Project: Amazon Fires doesn't have a linked github repo\n",
    "github_urls_list_2019.insert(2, 'https://github.com/oceanhackweek/DataAccess/tree/master/isopy')\n",
    "github_urls_list_2019.insert(6, 'None')\n",
    "github_urls_list_2019.append('https://github.com/oceanhackweek/DataAccess/tree/master/Chlorophyll')\n",
    "github_urls_list_2019.append('None')\n",
    "print(len(github_urls_list_2019))\n",
    "print(len(projects_list_2019))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cfe7fbd-3cab-4a70-9dc9-a9f15f2fe84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "11\n",
      "11\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "github_urls_list_2018 = get_github_url_list(soup_2018, tag=\"a\")\n",
    "print(len(github_urls_list_2018))\n",
    "print(len(projects_list_2018))\n",
    "# github link for Project: Mussel Beach is https://github.com/oceanhackweek/ohw2018_musselbeach\n",
    "# github link for Project: LTER Visualization is https://github.com/oceanhackweek/ohw_lter_vis\n",
    "# github link for Project: OOI Data Validation is https://github.com/oceanhackweek/ohw2018_Data_Validation\n",
    "github_urls_list_2018.insert(4, 'https://github.com/oceanhackweek/ohw2018_musselbeach')\n",
    "github_urls_list_2018.insert(7, 'https://github.com/oceanhackweek/ohw_lter_vis')\n",
    "github_urls_list_2018.insert(8, 'https://github.com/oceanhackweek/ohw2018_Data_Validation')\n",
    "print(len(github_urls_list_2018))\n",
    "print(len(projects_list_2018))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93776d03-a463-42b9-a482-b49f07a5ba80",
   "metadata": {},
   "source": [
    "##### create a dataframe from the two lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ccbe0a1-b2aa-4682-85bd-bc10728379ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2023 = pd.DataFrame(\n",
    "    {'project': projects_list_2023,\n",
    "     'github_url': github_urls_list_2023,\n",
    "     'year': 2023\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4db33ee0-a04b-46fe-a38e-06a3a3b3f7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2022 = pd.DataFrame(\n",
    "    {'project': projects_list_2022,\n",
    "     'github_url': github_urls_list_2022,\n",
    "     'year': 2022\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68d87005-4989-4808-bacb-fb0d90f9bc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2021 = pd.DataFrame(\n",
    "    {'project': projects_list_2021,\n",
    "     'github_url': github_urls_list_2021,\n",
    "     'year': 2021\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1d0b6e1-6d40-48ee-a42a-170000d1f2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2020 = pd.DataFrame(\n",
    "    {'project': projects_list_2020,\n",
    "     'github_url': github_urls_list_2020,\n",
    "     'year': 2020\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27a0d881-6ceb-46f4-aca4-439310568114",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2019 = pd.DataFrame(\n",
    "    {'project': projects_list_2019,\n",
    "     'github_url': github_urls_list_2019,\n",
    "     'year': 2019\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97434d84-4230-44a7-acda-722da9e9d6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018 = pd.DataFrame(\n",
    "    {'project': projects_list_2018,\n",
    "     'github_url': github_urls_list_2018,\n",
    "     'year': 2018\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0910b7c-4f32-455c-916a-ab0d2e697979",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [df_2023, df_2022, df_2021, df_2020, df_2019, df_2018]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "043ae09e-806e-4986-b6c2-f59692968054",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e1301b5-cc42-4f04-ab86-40b0527ab908",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['md'] = '[' + df['project'] + ']' + '(' + df['github_url'] + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e203ca28-36a3-4755-b7c3-834a05646b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>project</th>\n",
       "      <th>github_url</th>\n",
       "      <th>year</th>\n",
       "      <th>md</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Oil spill monitoring: segmentation of satellit...</td>\n",
       "      <td>https://github.com/oceanhackweek/ohw23_proj_oil</td>\n",
       "      <td>2023</td>\n",
       "      <td>[Oil spill monitoring: segmentation of satelli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Marine species distribution modeling tutorial:...</td>\n",
       "      <td>https://github.com/oceanhackweek/ohw23_proj_ma...</td>\n",
       "      <td>2023</td>\n",
       "      <td>[Marine species distribution modeling tutorial...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Inertial oscillations in the marginal ice zone</td>\n",
       "      <td>https://github.com/oceanhackweek/ohw23_proj_se...</td>\n",
       "      <td>2023</td>\n",
       "      <td>[Inertial oscillations in the marginal ice zon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Machine learning for Argo Data QC</td>\n",
       "      <td>https://github.com/oceanhackweek/ohw23_proj_ar...</td>\n",
       "      <td>2023</td>\n",
       "      <td>[Machine learning for Argo Data QC](https://gi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Benthic habitat mapping (image processing/seab...</td>\n",
       "      <td>https://github.com/oceanhackweek/ohw23-proj-ha...</td>\n",
       "      <td>2023</td>\n",
       "      <td>[Benthic habitat mapping (image processing/sea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Project:  Profiles</td>\n",
       "      <td>https://github.com/oceanhackweek/ohw18_profiles</td>\n",
       "      <td>2018</td>\n",
       "      <td>[Project:  Profiles](https://github.com/oceanh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Project:  LTER Visualization</td>\n",
       "      <td>https://github.com/oceanhackweek/ohw_lter_vis</td>\n",
       "      <td>2018</td>\n",
       "      <td>[Project:  LTER Visualization](https://github....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Project:  OOI Data Validation</td>\n",
       "      <td>https://github.com/oceanhackweek/ohw2018_Data_...</td>\n",
       "      <td>2018</td>\n",
       "      <td>[Project:  OOI Data Validation](https://github...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Project: Shallow Profiler Motion</td>\n",
       "      <td>https://github.com/oceanhackweek/ohw18_shallow...</td>\n",
       "      <td>2018</td>\n",
       "      <td>[Project: Shallow Profiler Motion](https://git...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Project:  ERDDAP Explorer</td>\n",
       "      <td>https://github.com/oceanhackweek/ohw18_erddap-...</td>\n",
       "      <td>2018</td>\n",
       "      <td>[Project:  ERDDAP Explorer](https://github.com...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              project  \\\n",
       "0   Oil spill monitoring: segmentation of satellit...   \n",
       "1   Marine species distribution modeling tutorial:...   \n",
       "2      Inertial oscillations in the marginal ice zone   \n",
       "3                   Machine learning for Argo Data QC   \n",
       "4   Benthic habitat mapping (image processing/seab...   \n",
       "..                                                ...   \n",
       "6                                  Project:  Profiles   \n",
       "7                        Project:  LTER Visualization   \n",
       "8                       Project:  OOI Data Validation   \n",
       "9                    Project: Shallow Profiler Motion   \n",
       "10                          Project:  ERDDAP Explorer   \n",
       "\n",
       "                                           github_url  year  \\\n",
       "0     https://github.com/oceanhackweek/ohw23_proj_oil  2023   \n",
       "1   https://github.com/oceanhackweek/ohw23_proj_ma...  2023   \n",
       "2   https://github.com/oceanhackweek/ohw23_proj_se...  2023   \n",
       "3   https://github.com/oceanhackweek/ohw23_proj_ar...  2023   \n",
       "4   https://github.com/oceanhackweek/ohw23-proj-ha...  2023   \n",
       "..                                                ...   ...   \n",
       "6     https://github.com/oceanhackweek/ohw18_profiles  2018   \n",
       "7       https://github.com/oceanhackweek/ohw_lter_vis  2018   \n",
       "8   https://github.com/oceanhackweek/ohw2018_Data_...  2018   \n",
       "9   https://github.com/oceanhackweek/ohw18_shallow...  2018   \n",
       "10  https://github.com/oceanhackweek/ohw18_erddap-...  2018   \n",
       "\n",
       "                                                   md  \n",
       "0   [Oil spill monitoring: segmentation of satelli...  \n",
       "1   [Marine species distribution modeling tutorial...  \n",
       "2   [Inertial oscillations in the marginal ice zon...  \n",
       "3   [Machine learning for Argo Data QC](https://gi...  \n",
       "4   [Benthic habitat mapping (image processing/sea...  \n",
       "..                                                ...  \n",
       "6   [Project:  Profiles](https://github.com/oceanh...  \n",
       "7   [Project:  LTER Visualization](https://github....  \n",
       "8   [Project:  OOI Data Validation](https://github...  \n",
       "9   [Project: Shallow Profiler Motion](https://git...  \n",
       "10  [Project:  ERDDAP Explorer](https://github.com...  \n",
       "\n",
       "[72 rows x 4 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943aa5c2-d564-4158-bf47-3a8c869cb926",
   "metadata": {},
   "source": [
    "##### write out a csv from that dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9009dc3f-baea-4593-848a-57f80d38b4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('project_list_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fc4cfc-dfd2-47c1-b3a7-efdfe17d34c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
